{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. loading the dataset\n",
        "- custom dataset spcially designed for the Homonyms Problem"
      ],
      "metadata": {
        "id": "EPpdXZzSmrEq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5HkTmIkJcN2E"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import torch\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = 'dataset.json'\n",
        "# Open and load the JSON data\n",
        "with open(dataset_path, 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "fhVVj9o7jTgH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data)\n",
        "# represent labels as numbers for sklearn\n",
        "df['label_numeric'] = df['label'].apply(lambda x: 1 if x == 'positive' else 0)\n",
        "print(\"\\nFirst 5 rows of the DataFrame:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwqSGww4jYc2",
        "outputId": "4945108f-9c76-40bf-ef00-e61f57d27c46"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 5 rows of the DataFrame:\n",
            "                                           Sentence     label  label_numeric\n",
            "0            He broke the bank and lost everything.  negative              0\n",
            "1  She broke the bank with her amazing performance.  positive              1\n",
            "2            The mouse in my kitchen ruined my day.  negative              0\n",
            "3    The new mouse for my computer works perfectly.  positive              1\n",
            "4                   He left the party feeling blue.  negative              0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To ensure examples mentioned in the task are in the test dataset (they are not in the json file)\n",
        "challenge_data = [\n",
        "    {\"Sentence\": \"I hate the selfishness in you\", \"label\": \"negative\"},\n",
        "    {\"Sentence\": \"I hate any one who can hurt you\", \"label\": \"positive\"}\n",
        "]\n",
        "challenge_df = pd.DataFrame(challenge_data)\n",
        "challenge_df['label_numeric'] = challenge_df['label'].apply(lambda x: 1 if x == 'positive' else 0)"
      ],
      "metadata": {
        "id": "aUk1043brrvO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the dataset to training and initial test\n",
        "train_df, init_test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=df['label_numeric']\n",
        ")\n",
        "\n",
        "print(f\"\\nSplit main data into {len(train_df)} training and {len(init_test_df)} initial test examples.\")\n",
        "\n",
        "# Add the challenge data to the test dataset\n",
        "test_df = pd.concat([init_test_df, challenge_df], ignore_index=True)\n",
        "\n",
        "print(f\"Final test set created with {len(test_df)} examples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt-TUAJnjiaR",
        "outputId": "70d3d3a9-e071-44ad-c169-e2ae3d2f5db8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Split main data into 45 training and 15 initial test examples.\n",
            "Final test set created with 17 examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finalize datasets\n",
        "X_train = train_df['Sentence']\n",
        "y_train = train_df['label_numeric']\n",
        "X_test = test_df['Sentence']\n",
        "y_test = test_df['label_numeric']\n"
      ],
      "metadata": {
        "id": "rPd2mvS8sP5E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. The Baseline:GloVe Embeddings + classifier"
      ],
      "metadata": {
        "id": "EVsKQTiGnolW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Pre-trained GloVe Embeddings\n",
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9WABpGXnv7u",
        "outputId": "ae5f5e58-4c12-48f3-cd19-2901de3363a5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-04 10:20:25--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-10-04 10:20:25--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1        5%[>                   ]  41.29M  5.07MB/s    eta 63s    ^C\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GloVe Vectors\n",
        "# I chose to use 200 dimention GloVe embedding as a middle ground\n",
        "glove_path = 'glove.6B.200d.txt'\n",
        "embedding_dim = 200\n",
        "\n",
        "# map words to their vector representations\n",
        "embedding_dict = {}\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_dict[word] = vector"
      ],
      "metadata": {
        "id": "43dLOnODnkrc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_vector(sentence, embedding_dict, dim=200):\n",
        "    \"\"\"\n",
        "    Converts a sentence to its average GloVe vector representation.\n",
        "    \"\"\"\n",
        "    # Split the sentence into words (tokens)\n",
        "    words = sentence.lower().split()\n",
        "\n",
        "    # Initialize an empty list to store the vectors of words found in the dictionary\n",
        "    word_vectors = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in embedding_dict:\n",
        "            word_vectors.append(embedding_dict[word])\n",
        "\n",
        "    # If no words in the sentence are in our GloVe dictionary, return a zero vector\n",
        "    if not word_vectors:\n",
        "        return np.zeros(dim)\n",
        "\n",
        "    # Calculate the mean of the word vectors to get the sentence vector\n",
        "    sentence_vector = np.mean(word_vectors, axis=0)\n",
        "\n",
        "    return sentence_vector\n",
        "\n",
        "\n",
        "X_train_glove = np.array([sentence_to_vector(s, embedding_dict) for s in X_train])\n",
        "X_test_glove = np.array([sentence_to_vector(s, embedding_dict) for s in X_test])\n",
        "\n",
        "print(\"\\nConverted all sentences to GloVe vectors.\")\n",
        "print(f\"Shape of training vectors: {X_train_glove.shape}\")\n",
        "print(f\"Shape of testing vectors: {X_test_glove.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQSCKJRToPZw",
        "outputId": "257fe9c6-ecd6-433c-b7c5-6be674e63a86"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converted all sentences to GloVe vectors.\n",
            "Shape of training vectors: (45, 200)\n",
            "Shape of testing vectors: (17, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the classifier\n",
        "glove_classifier = LogisticRegression(random_state=42)\n",
        "\n",
        "print(\"\\nTraining the Logistic Regression classifier on GloVe vector\")\n",
        "\n",
        "# Train the model\n",
        "glove_classifier.fit(X_train_glove, y_train)\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNe-Qdteo1Ub",
        "outputId": "548f9430-1bb7-47af-e2eb-aab797420cb4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the Logistic Regression classifier on GloVe vector\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Evaluating the GloVe Baseline Model ---\")\n",
        "\n",
        "# Make predictions on the full test set\n",
        "y_pred_glove = glove_classifier.predict(X_test_glove)\n",
        "\n",
        "print(\"\\nClassification Report (Overall Performance on Test Set):\")\n",
        "print(\"-\" * 60)\n",
        "print(classification_report(y_test, y_pred_glove, target_names=['negative', 'positive']))\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'Sentence': X_test,\n",
        "    'True_Label': y_test.map({0: 'negative', 1: 'positive'}),\n",
        "    'Predicted_Label': pd.Series(y_pred_glove, index=X_test.index).map({0: 'negative', 1: 'positive'})\n",
        "})\n",
        "results_df['Correct'] = results_df['True_Label'] == results_df['Predicted_Label']\n",
        "# evaluate on the challenge sentences mentiones in the task\n",
        "challenge_sentence_1 = \"I hate the selfishness in you\"\n",
        "challenge_sentence_2 = \"I hate any one who can hurt you\"\n",
        "challenge_results_df = results_df[\n",
        "    results_df['Sentence'].isin([challenge_sentence_1, challenge_sentence_2])\n",
        "]\n",
        "\n",
        "print(\"\\n\\nAnalysis of Performance on Core Challenge Sentences:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "if challenge_results_df.empty:\n",
        "    print(\"WARNING: The challenge sentences were not found in the test set.\")\n",
        "else:\n",
        "    print(challenge_results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pQBxa7KpC8f",
        "outputId": "110714a8-b086-4cad-975a-c72a523cc52c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating the GloVe Baseline Model ---\n",
            "\n",
            "Classification Report (Overall Performance on Test Set):\n",
            "------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.89      0.84         9\n",
            "    positive       0.86      0.75      0.80         8\n",
            "\n",
            "    accuracy                           0.82        17\n",
            "   macro avg       0.83      0.82      0.82        17\n",
            "weighted avg       0.83      0.82      0.82        17\n",
            "\n",
            "\n",
            "\n",
            "Analysis of Performance on Core Challenge Sentences:\n",
            "------------------------------------------------------------\n",
            "                           Sentence True_Label Predicted_Label  Correct\n",
            "15    I hate the selfishness in you   negative        negative     True\n",
            "16  I hate any one who can hurt you   positive        negative    False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT And it's variants contextual Solution"
      ],
      "metadata": {
        "id": "g5WuKOwXphtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_to_test = {\n",
        "    \"BERT\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    \"RoBERTa (Sentiment)\": \"siebert/sentiment-roberta-large-english\"\n",
        "}"
      ],
      "metadata": {
        "id": "-KQGok41qMi2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_pipelines = {}\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"loading all Sentiment models\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 3. Loop through the models and load each one\n",
        "for model_name, model_checkpoint in models_to_test.items():\n",
        "    print(f\"\\nLoading model: {model_name}...\")\n",
        "    try:\n",
        "        # Load the pipeline and store it in our dictionary\n",
        "        loaded_pipelines[model_name] = pipeline(\n",
        "            \"sentiment-analysis\",\n",
        "            model=model_checkpoint,\n",
        "            device=0  # Use GPU if available\n",
        "        )\n",
        "        print(f\"--> Successfully loaded '{model_name}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"--> FAILED to load model {model_checkpoint}. Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9kCL58n3nGv",
        "outputId": "06b8d731-f94c-4769-f29a-6f5a0b7228c1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "loading all Sentiment models\n",
            "============================================================\n",
            "\n",
            "Loading model: BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Successfully loaded 'BERT'.\n",
            "\n",
            "Loading model: RoBERTa (Sentiment)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Successfully loaded 'RoBERTa (Sentiment)'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "challenge_sentence_1 = \"I hate the selfishness in you\"\n",
        "challenge_sentence_2 = \"I hate any one who can hurt you\"\n",
        "\n",
        "# Convert the test data to a list for the pipeline\n",
        "test_sentences = X_test.tolist()\n",
        "\n",
        "# 2. Loop through each pre-loaded model and evaluate it\n",
        "for model_name, sentiment_analyzer in loaded_pipelines.items():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"EVALUATING MODEL: {model_name}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # A. Run inference on the entire test set\n",
        "    print(f\"Running inference on {len(test_sentences)} test sentences...\")\n",
        "    predictions_raw = sentiment_analyzer(test_sentences)\n",
        "    print(\"Inference complete.\")\n",
        "\n",
        "    # B. Process the predictions to a consistent format\n",
        "    predicted_labels = []\n",
        "    for pred in predictions_raw:\n",
        "        label_norm = pred['label'].lower()\n",
        "        if not (label_norm == 'positive' or label_norm == 'negative'):\n",
        "            if label_norm in ['label_2', 'label_1', 'pos']:\n",
        "                 label_norm = 'positive'\n",
        "            else:\n",
        "                 label_norm = 'negative'\n",
        "        predicted_labels.append(label_norm)\n",
        "\n",
        "    predicted_numeric = [1 if label == 'positive' else 0 for label in predicted_labels]\n",
        "\n",
        "    # C. Calculate and display overall performance metrics\n",
        "    accuracy = accuracy_score(y_test, predicted_numeric)\n",
        "    report = classification_report(y_test, predicted_numeric, target_names=['negative', 'positive'])\n",
        "\n",
        "    print(f\"\\n--- Overall Performance for {model_name} ---\")\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    # D. Analyze and display performance on the two core challenge sentences\n",
        "    results_df = pd.DataFrame({\n",
        "        'Sentence': X_test,\n",
        "        'True_Label': y_test.map({0: 'negative', 1: 'positive'}),\n",
        "        'Prediction': predicted_labels\n",
        "    })\n",
        "\n",
        "    challenge_results = results_df[\n",
        "        results_df['Sentence'].isin([challenge_sentence_1, challenge_sentence_2])\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n--- Analysis of {model_name} on Core Challenge Sentences ---\")\n",
        "    if challenge_results.empty:\n",
        "        print(\"WARNING: The challenge sentences were not found in the test set.\")\n",
        "    else:\n",
        "        # Add a correctness column for clarity in this specific output\n",
        "        challenge_results['Correct'] = challenge_results['True_Label'] == challenge_results['Prediction']\n",
        "        print(challenge_results.to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cTl3bkjwj0X",
        "outputId": "c35fc418-39e3-47b7-ce5c-734b22e63e19"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "EVALUATING MODEL: BERT\n",
            "================================================================================\n",
            "Running inference on 17 test sentences...\n",
            "Inference complete.\n",
            "\n",
            "--- Overall Performance for BERT ---\n",
            "Accuracy: 94.12%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      1.00      0.95         9\n",
            "    positive       1.00      0.88      0.93         8\n",
            "\n",
            "    accuracy                           0.94        17\n",
            "   macro avg       0.95      0.94      0.94        17\n",
            "weighted avg       0.95      0.94      0.94        17\n",
            "\n",
            "\n",
            "--- Analysis of BERT on Core Challenge Sentences ---\n",
            "                           Sentence True_Label Prediction  Correct\n",
            "15    I hate the selfishness in you   negative   negative     True\n",
            "16  I hate any one who can hurt you   positive   negative    False\n",
            "\n",
            "================================================================================\n",
            "EVALUATING MODEL: RoBERTa (Sentiment)\n",
            "================================================================================\n",
            "Running inference on 17 test sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1574393188.py:56: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  challenge_results['Correct'] = challenge_results['True_Label'] == challenge_results['Prediction']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference complete.\n",
            "\n",
            "--- Overall Performance for RoBERTa (Sentiment) ---\n",
            "Accuracy: 94.12%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      1.00      0.95         9\n",
            "    positive       1.00      0.88      0.93         8\n",
            "\n",
            "    accuracy                           0.94        17\n",
            "   macro avg       0.95      0.94      0.94        17\n",
            "weighted avg       0.95      0.94      0.94        17\n",
            "\n",
            "\n",
            "--- Analysis of RoBERTa (Sentiment) on Core Challenge Sentences ---\n",
            "                           Sentence True_Label Prediction  Correct\n",
            "15    I hate the selfishness in you   negative   negative     True\n",
            "16  I hate any one who can hurt you   positive   negative    False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1574393188.py:56: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  challenge_results['Correct'] = challenge_results['True_Label'] == challenge_results['Prediction']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"I hate any one who can hurt you\" Augmentation\n",
        "As shown on the last cell all advanced Sentiment models failed with \"I hate any one who can hurt you\" as it lakes context, in the nexts cells we will try to enhance the sentence with some context to see it's effect on the classification"
      ],
      "metadata": {
        "id": "9PwgK9mnqWbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_to_analyze = [\n",
        "    \"I hate any one who can hurt you\", # Baseline\n",
        "    \"I love you and I hate any one who can hurt you\", # Strong Emotional Anchor\n",
        "    \"you are my friend I hate any one who can hurt you\", # Relational Anchor\n",
        "    \"Here is my opinion: I hate any one who can hurt you\", # Neutral Framing\n",
        "]"
      ],
      "metadata": {
        "id": "lKzVDlAN3ytz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_results = []\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Runing sentences_to_analyze on all models\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# loop through the PRE-LOADED models and test each sentence\n",
        "for model_name, sentiment_analyzer in loaded_pipelines.items():\n",
        "    print(f\"\\nTesting with model: {model_name}\")\n",
        "\n",
        "    # Run inference on the entire list of sentences at once for efficiency\n",
        "    predictions_raw = sentiment_analyzer(sentences_to_analyze)\n",
        "\n",
        "    # Process each result\n",
        "    for sentence, result_raw in zip(sentences_to_analyze, predictions_raw):\n",
        "        # Normalize the label to a consistent 'positive' or 'negative'\n",
        "        label_norm = result_raw['label'].lower()\n",
        "        if not (label_norm == 'positive' or label_norm == 'negative'):\n",
        "            if label_norm in ['label_2', 'label_1', 'pos']:\n",
        "                 label_norm = 'positive'\n",
        "            else:\n",
        "                 label_norm = 'negative'\n",
        "\n",
        "        score = result_raw['score']\n",
        "\n",
        "        # Store the detailed result\n",
        "        all_results.append({\n",
        "            'Model': model_name,\n",
        "            'Sentence': sentence,\n",
        "            'Prediction': label_norm,\n",
        "            'Confidence': f\"{score:.2%}\"\n",
        "        })\n",
        "\n",
        "# display the Final, Comprehensive Summary Table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL SUMMARY: IMPACT OF DIFFERENT CONTEXTUAL ANCHORS ACROSS MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary_df = pd.DataFrame(all_results)\n",
        "summary_df['Display_Result'] = summary_df['Prediction'] + \" (\" + summary_df['Confidence'] + \")\"\n",
        "\n",
        "final_pivot = summary_df.pivot_table(\n",
        "    index='Sentence',\n",
        "    columns='Model',\n",
        "    values='Display_Result',\n",
        "    aggfunc='first'\n",
        ")\n",
        "\n",
        "# Define a logical order for columns and rows\n",
        "column_order = [name for name in models_to_test.keys() if name in final_pivot.columns]\n",
        "final_pivot = final_pivot.reindex(columns=column_order)\n",
        "final_pivot = final_pivot.reindex(sentences_to_analyze)\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.width', 120)\n",
        "\n",
        "print(final_pivot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91kNiX0LyYVa",
        "outputId": "e5b408e4-0186-4da6-fa13-adc03d14675b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Runing sentences_to_analyze on all models\n",
            "================================================================================\n",
            "\n",
            "Testing with model: BERT\n",
            "\n",
            "Testing with model: RoBERTa (Sentiment)\n",
            "\n",
            "================================================================================\n",
            "FINAL SUMMARY: IMPACT OF DIFFERENT CONTEXTUAL ANCHORS ACROSS MODELS\n",
            "================================================================================\n",
            "Model                                                             BERT RoBERTa (Sentiment)\n",
            "Sentence                                                                                  \n",
            "I hate any one who can hurt you                      negative (98.15%)   negative (99.41%)\n",
            "I love you and I hate any one who can hurt you       positive (99.92%)   positive (99.75%)\n",
            "you are my friend I hate any one who can hurt you    positive (99.84%)   positive (99.53%)\n",
            "Here is my opinion: I hate any one who can hurt you  negative (93.77%)   negative (99.23%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Evaluating the GloVe Baseline Model ---\")\n",
        "\n",
        "# Convert the standard test sentences to GloVe vectors\n",
        "X_test_glove = np.array([sentence_to_vector(s, embedding_dict) for s in X_test])\n",
        "# Make predictions\n",
        "y_pred_glove = glove_classifier.predict(X_test_glove)\n",
        "\n",
        "print(\"\\nClassification Report (Overall Performance on Standard Test Set):\")\n",
        "print(\"-\" * 60)\n",
        "print(classification_report(y_test, y_pred_glove, target_names=['negative', 'positive']))\n",
        "\n",
        "\n",
        "print(\"\\n\\nAnalysis of GloVe Performance on Challenge & Augmented Sentences:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Create a list to store the results for these specific sentences\n",
        "glove_specific_results = []\n",
        "\n",
        "for sentence in sentences_to_analyze:\n",
        "    # Convert the single sentence to its GloVe vector\n",
        "    sentence_vec = sentence_to_vector(sentence, embedding_dict).reshape(1, -1)\n",
        "\n",
        "    # Get the model's prediction (0 or 1)\n",
        "    prediction_numeric = glove_classifier.predict(sentence_vec)[0]\n",
        "\n",
        "    # Convert the numeric prediction to a string label\n",
        "    prediction_label = \"positive\" if prediction_numeric == 1 else \"negative\"\n",
        "\n",
        "    glove_specific_results.append({\n",
        "        'Sentence': sentence,\n",
        "        'GloVe_Prediction': prediction_label\n",
        "    })\n",
        "\n",
        "# Convert the results to a DataFrame for clean printing\n",
        "glove_analysis_df = pd.DataFrame(glove_specific_results)\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(glove_analysis_df.to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ8dDF050_lk",
        "outputId": "640a97ce-ada3-4a7f-fda4-8c2641cc195a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating the GloVe Baseline Model ---\n",
            "\n",
            "Classification Report (Overall Performance on Standard Test Set):\n",
            "------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.89      0.84         9\n",
            "    positive       0.86      0.75      0.80         8\n",
            "\n",
            "    accuracy                           0.82        17\n",
            "   macro avg       0.83      0.82      0.82        17\n",
            "weighted avg       0.83      0.82      0.82        17\n",
            "\n",
            "\n",
            "\n",
            "Analysis of GloVe Performance on Challenge & Augmented Sentences:\n",
            "------------------------------------------------------------\n",
            "                                              Sentence GloVe_Prediction\n",
            "0                      I hate any one who can hurt you         negative\n",
            "1       I love you and I hate any one who can hurt you         positive\n",
            "2    you are my friend I hate any one who can hurt you         positive\n",
            "3  Here is my opinion: I hate any one who can hurt you         positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lw34v2_UzmAb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}